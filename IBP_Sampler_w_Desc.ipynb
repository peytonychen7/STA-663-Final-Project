{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section: Algorithim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our Algorithim is composed of 3 functions: Indian Buffet Process, log likelyhood function, and the Gibbs Sampler "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I. Indian Buffett Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Indian Buffett Process is used to initialize our Z matrix. The function is designed based on the following steps described in the paper. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Initial customer takes the first Poisson($\\alpha$) dishes  \n",
    "2. The following ith customers:  \n",
    "a. Take dishes that have previously been taken with probability of $\\frac{m_k}{i}$ where $m_k$ is the number of customers who           have tried the dish  \n",
    "b. Try Poisson($\\alpha$/i) number of new dishes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IBP(alpha,N):\n",
    "    \"\"\" \n",
    "    This is a function generate a binary matrix Z using Indian Buffett Process.\n",
    "    \n",
    "    The inputs are:\n",
    "    \n",
    "    alpha: Initial parameter for Possion distribution\n",
    "    \n",
    "    N: Number of objects to be used to generated Z\n",
    "    \"\"\"\n",
    "    k = alpha * N * 10\n",
    "    Z = np.zeros((N,k))\n",
    "    \n",
    "    #initial customer\n",
    "    d = np.random.poisson(alpha)\n",
    "    Z[0,0:d] = 1\n",
    "\n",
    "    k_new = d\n",
    "    #Rest of the customers\n",
    "    for i in range(1,N):\n",
    "        for j in range(k_new):\n",
    "            probability = np.sum(Z[0:i,j])/(i + 1)\n",
    "            if probability > np.random.random():\n",
    "                Z[i,j] = 1\n",
    "        d = np.random.poisson(alpha/(i + 1))\n",
    "        Z[i,k_new:k_new + d] = 1\n",
    "        k_new += d\n",
    "        \n",
    "    return Z[:,0:k_new]\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II. Log Likelyhood Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a helper function that is frequently referenced during the Gibbs Sampler. It it is the log of the likelyhood of p(X | Z, $\\sigma_X$, $\\sigma_A$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_likelyhood(X, N,D,K,sigma_X,sigma_A,Z):\n",
    "    '''\n",
    "    This is a helper function for the sampler that computes the log likelihood for the \n",
    "    linear-Gaussian bindary latent feature model.\n",
    "    \n",
    "    The parameters are:\n",
    "    \n",
    "    X: Data matrix\n",
    "    \n",
    "    N: Number of columns for X\n",
    "    \n",
    "    D: Number of rows for X\n",
    "    \n",
    "    K: Number of columns for Z\n",
    "    \n",
    "    Sigma_X: Standard deviation of X\n",
    "    \n",
    "    Sigma_A: Standard deviation of alpha\n",
    "    \n",
    "    Z: Binary matrix generated by Indian buffet process\n",
    "    '''\n",
    "    M = Z.T @ Z + (sigma_X**2/sigma_A**2)*np.eye(K)\n",
    "    part1 = N*D/2 * np.log(2*np.pi) + (N - K)*D*np.log(sigma_X) + K*D*np.log(sigma_A)+D/2*np.log(np.linalg.det(M))\n",
    "    part2_inside = np.eye(N) - (Z @ np.linalg.inv(M) @ Z.T)\n",
    "    part2 = -1/(2 * sigma_X**2) * np.trace(X.T @ part2_inside @ X)\n",
    "    return part2 - part1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III. Gibbs Sampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the main portion of our function and does the following steps:  \n",
    "1. For each $Z_{ik}$, sample to see if it is 0 or 1\n",
    "2. Sample to see if more columns should be added (increase k) or potentially be removed\n",
    "3. Update $\\sigma_X$ as needed\n",
    "4. Update $\\sigma_A$ as needed\n",
    "5. Compute new $\\alpha$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampler(X,alpha,niter,epsilon,sigma_X,sigma_A,alpha_a_prior,alpha_b_prior,max_new):\n",
    "    '''\n",
    "    This function performs a Gibbs sampler using the binary matrix Z generated by Indian buffet process and a \n",
    "    log likelihood function for the linear-Gaussian bindary latent feature model.\n",
    "    \n",
    "    The parameters are:\n",
    "    \n",
    "    X: Data matrix\n",
    "    \n",
    "    alpha: parameter for the Possion distribution that is used to generate a binary matrix Z using Indian buffet process\n",
    "    \n",
    "    niter: The number of iterations for the sampler\n",
    "    \n",
    "    Sigma_X: Standard deviation of X\n",
    "    \n",
    "    Sigma_A: Standard deviation of alpha\n",
    "    \n",
    "    alpha_a_prior: Shape hyperparameter for the prior distribution of alpha, which follows a Gamma distribution.\n",
    "    \n",
    "    alpha_b_prior: Rate hyperparameter for the prior distribution of alpha, which follows a Gamma distribution.\n",
    "\n",
    "    max_new: Maximum number of new K's per iteration\n",
    "    \n",
    "    '''\n",
    "    N = X.shape[0]\n",
    "    D = X.shape[1]\n",
    "    Z = IBP(alpha,N) # set inital Z\n",
    "    K = Z.shape[1]\n",
    "    K_values = np.zeros(niter)\n",
    "    alpha_values = np.zeros(niter)\n",
    "    Sigma_X_values = np.zeros(niter)\n",
    "    Sigma_A_values = np.zeros(niter)\n",
    "    HN = 0\n",
    "    for i in range(1,N+1):\n",
    "        HN += 1.0/i\n",
    "    for runs in range(niter):\n",
    "        for i in range(N):\n",
    "            for j in range(K):\n",
    "                #Sample Z given conditionals\n",
    "                \n",
    "                col_k_count = sum(Z[:,j]) - Z[i,j] #p(zik|z-ik) = 0 so we set to 0\n",
    "                if col_k_count == 0:\n",
    "                    Z[i,j] = 0\n",
    "                    \n",
    "                else:\n",
    "                    Z[i,j] = 0\n",
    "                    Z0_p = log_likelyhood(X,N,D,K,sigma_X,sigma_A,Z) + np.log(N - col_k_count)\n",
    "                    Z[i,j] = 1\n",
    "                    Z1_p = log_likelyhood(X,N,D,K,sigma_X,sigma_A,Z) + np.log(col_k_count)\n",
    "                    L = Z1_p - Z0_p\n",
    "                    if L > 40: #helps with overflow\n",
    "                        Z[i,j] = 1\n",
    "                    elif L < -40:\n",
    "                        Z[i,j] = 0\n",
    "                    elif np.exp(L)/(1 + np.exp(L)) > np.random.random():\n",
    "                        Z[i,j] = 1\n",
    "                    else:\n",
    "                        Z[i,j] = 0\n",
    "                        \n",
    "            #Sample to see if new columns get added\n",
    "            log_prob = np.zeros(max_new)\n",
    "            a_N = alpha/N\n",
    "            log_prob[0] = -a_N + log_likelyhood(X,N,D,Z.shape[1],sigma_X,sigma_A,Z)\n",
    "            for new_ks in range(1,max_new):\n",
    "                new_cols = np.zeros((N,new_ks))\n",
    "                new_cols[i,:] = 1\n",
    "                Z_new = np.hstack((Z,new_cols))\n",
    "                #Poisson(alpha/n) * log likelyhood\n",
    "                log_prob[new_ks] = new_ks*np.log(a_N) - a_N - np.log(math.factorial(new_ks)) + log_likelyhood(X,N,D,Z_new.shape[1],sigma_X,sigma_A,Z_new)\n",
    "            #Convert log likelyhoods\n",
    "            prob = np.exp(log_prob - max(log_prob))\n",
    "            prob = prob/sum(prob)\n",
    "\n",
    "            #Sample probabilites and add columns accordingly\n",
    "            new_cols_add = list(np.random.multinomial(1,prob) == 1).index(1)\n",
    "            col_k_count = np.sum(Z,axis = 0) - Z[i,:]\n",
    "            if new_cols_add == 0:\n",
    "                Z = Z[:,col_k_count != 0]\n",
    "            else:\n",
    "                newcols = np.zeros((N,new_cols_add))\n",
    "                newcols[i,:] = 1\n",
    "                Z = np.hstack((Z[:,col_k_count != 0],newcols))\n",
    "            K = Z.shape[1]\n",
    "        \n",
    "        #Part2\n",
    "        current_likelyhood = log_likelyhood(X,N,D,K,sigma_X,sigma_A,Z) \n",
    "        \n",
    "        #Sigma_X\n",
    "        sigma_X_new = sigma_X + np.random.uniform(-epsilon,epsilon)\n",
    "        new_likelyhood = log_likelyhood(X,N,D,K,sigma_X_new,sigma_A,Z)\n",
    "        if new_likelyhood - current_likelyhood >= 0:\n",
    "            sigma_X = sigma_X_new\n",
    "        elif np.exp(new_likelyhood - current_likelyhood) > np.random.random():\n",
    "            sigma_X = sigma_X_new\n",
    "        else:\n",
    "            sigma_X = sigma_X\n",
    "            \n",
    "        #Sigma_A\n",
    "        sigma_A_new = sigma_A + np.random.uniform(-epsilon,epsilon)\n",
    "        new_log_likelyhood = log_likelyhood(X,N,D,K,sigma_X,sigma_A_new,Z)\n",
    "        if new_likelyhood - current_likelyhood >= 0:\n",
    "            sigma_A = sigma_A_new\n",
    "        elif np.exp(new_likelyhood - current_likelyhood) > np.random.random():\n",
    "            sigma_A = sigma_A_new\n",
    "        else:\n",
    "            sigma_A = sigma_A\n",
    "         \n",
    "        #Alpha\n",
    "        alpha = np.random.gamma(alpha_a_prior + K,alpha_b_prior + 1/(1 + HN))\n",
    "        \n",
    "        K_values[runs] = K\n",
    "        alpha_values[runs] = alpha\n",
    "        Sigma_X_values[runs] = sigma_X\n",
    "        Sigma_A_values[runs] = sigma_A\n",
    "        # print(runs,K,sigma_X)\n",
    "    return(K_values,alpha_values,Sigma_X_values,Sigma_A_values,Z)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
