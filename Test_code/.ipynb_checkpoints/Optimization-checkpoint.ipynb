{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Optimization\n",
    "\n",
    "In this section, we explore a few optimization methods to make the sampler more efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "S:\\Anaconda\\lib\\site-packages\\setuptools\\distutils_patch.py:25: UserWarning: Distutils was imported before Setuptools. This usage is discouraged and may exhibit undesirable behaviors or errors. Please use Setuptools' objects directly or at least import Setuptools first.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import setuptools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure the package is installed. Import functions from the package. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: STA-663-IBP==0.1 in s:\\anaconda\\lib\\site-packages (0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install STA-663-IBP==0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from example_pkg.IBP_Sampler import IBP, log_likelyhood, sampler\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import math\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test the performance, we used the example images used in Ilker Yildirim’s paper as my simulation data set (Yildirim 2012)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkkAAACoCAYAAAD0B3o6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAASIklEQVR4nO3dfcwlZX3G8etiFwXlrZEVcNmFVvq2lVa6p9h0tVU0hjeFxrQVjRAVV9tiIJFQ/ugLtNI0TbUkRrQgBlqwSBASqiKSClipAs8D1LIsNIjbsLDAbpGXbVBc/PWPmacelnv3zDxzzzMz53w/yUnOyzDnN+dcO/yee+bc44gQAAAAXmyPrgsAAADoI5okAACABJokAACABJokAACABJokAACABJokAACAhJltkmzfYPu0ruvAcJEh5ECO0BQZak/vmyTbm2w/Z3u77R/Y/ortVU3XGxHHRcTlNWs53HbYXt70/dtk+3W2b7S9zfbMT4RFhuqzfZrtedvP2N5s+2/7XnPbyFF9tt9t+wHbT9t+wvbltvfruq6ukKFmbH9jqWvufZNUekdE7CPpEEmPS/pUx/X03Y8lXS3pg10X0iNkqJ5XSDpL0oGS3iDprZLO7rSifiBH9dwmaV1E7C/p5yQtl/TxbkvqHBlaBNvvVZGfJTWUJkmSFBE/lHSNpDULz9k+wfbd5V+8D9s+b+y1vWxfYft/bD9l+07bB5Wv3WL79PL+EbZvLf/a2Wb7i1XqsX2Z7YvKoc7ttm+zfbDtC8u/Eu63fdTY8ufa/p7tZ23fZ/t3x15bZvsT5ft/3/YZ4x2z7f1tX2p7i+1HbH/c9rJdfE4PRMSlkjbU+XxnARmqnKHPRMS/RcTzEfGIpCslravxUU81clQ5Rw9HxLaxp16QdESVbZp2ZKhahhaWl/QXks6p+PFmM6gmyfYrJP2BpO+MPf2/kk6VdICkEyT9oe2Ty9dOk7S/pFWSXiXpI5KeS6z6ryR9XdLPSDpU9Tr735f0pyr+4v6RpG9Luqt8fI2kT44t+z1JbyprOl/SFbYPKV/7kKTjJL1e0q9LOlkvdrmkHSp2MEdJeruk02vUCZEhLT5Dvy2a7v9HjqrnyPYbbT8t6VlJ75J0YY1tmlpkqNa+6K8lfUbSYzW2JY+I6PVN0iZJ2yU9peJDfVTSkbtZ/kJJf1/e/4Ckf5f0q4nlbpF0enn/HyVdLOnQCbUcLikkLS8fXybpkrHXPypp49jjIyU9tZv13SPppPL+NyR9eOy1ty28l6SDVAR277HXT5F084R6jyi+4u6/RzI0zAyVy71f0mZJB3b9XZKjQedopaTzJP1C198lGRpOhiSNynUv37nmpbgNZSTp5Ig4QNLLJZ0h6VbbB0uS7TfYvtn21vKvlY+o6Hol6Z8k3SjpKtuPujj5dM/E+s+RZEl32N5g+wM1ant87P5zicf7LDywfarte8qh0qckvW6s1tdIenjsvx2/f5ikPSVtGftv/0HSq2vUOevI0CIyVP4V+zeSjosXHzaZVeRokfuiKA7bfk3SVdU3aSqRoYoZsr2HpIsknRkRO2psRzZDaZIkSRHxQkRcq+K49hvLp78g6XpJq6I4OfCzKgKiiPhxRJwfEWsk/ZakE1UMZe683sci4kMR8RpJH5Z0ke2sx81tHybpEhX/KF5V/iO5d6FWSVtUDI0uGP/Fw8MqOu8DI+KA8rZfRPxKzhpnARmqniHbx5bv946I+M+MmzJ45GjR+6Llkl7baAOmBBmqlKH9VIwkfdH2Y5LuLJ/fbPtN+bZo1wbVJLlwkopjrRvLp/eV9GRE/ND20ZLeM7b8W2wfWZ4Q9oyKX329kFjv79le+EJ/oGI47yXLNfTKcr1by/d8v4rOe8HVks60vdL2AZL+ZOGFiNii4hjzJ2zvZ3sP26+1/TupNyo/p70kvax8vJftl2fenkEiQ5UzdIyKk7XfFRF3ZN6OwSNHlXP0Xtury8/rMEkXSPrXzNszSGSoUoaeVjEq9frydnz5/FpJt2fepqShNEn/Ynu7imBcIOm0iFg4ifSPJP2l7Wcl/bmKL2fBwSpONntGRQhvlXRFYv2/Ien28j2uVzG09/2cGxAR90n6hIoT4R5XcXz3trFFLlERnO9KulvSV1Ucs14I96kqmp77VAT/GhU/IU05TMXQ6MJn9JykBzJtylCRoXoZ+jMVJ2R+1cUvXbbbviHn9gwUOaqXozUqzqPZXr7HAypO6p1lZKhihqLw2MJNZVMm6fGIeD7nNu2KyxOj0DO2j5P02Yg4rOtaMExkCDmQIzQ15AwNZSRp6tne2/bxtpfbXqliTojruq4Lw0GGkAM5QlPTlCFGknrCxZwZt0r6JRWHx76iYpj0mU4Lw2CQIeRAjtDUNGWIJgkAACCBw20AAAAJrVwszpmvPL927dqcq5Mkzc/PZ1/nrIkIT15qcXJnCP3UZoak2cxRG/vLPtu0aZO2bdvGvghNbYuIFTs/2crhttyhaqnG7OucNTRJaIomKb9ZO4ViNBppbm6OfRGamo+I0c5PcrgNAAAggSYJAAAggSYJAAAggSYJAAAggSYJAAAgoVKTZPtY2w/YftD2uW0XhelDhpADOUIO5AhVTWySbC+T9GlJx6m4ovMptte0XRimBxlCDuQIOZAj1FFlJOloSQ9GxEMR8bykqySd1G5ZmDJkCDmQI+RAjlBZlSZppaSHxx5vLp8DqiJDyIEcIQdyhMqqXJYkNZPpS2Ygtb1e0vrGFWEakSHkQI6Qw8QckSEsqNIkbZa0auzxoZIe3XmhiLhY0sUS07jjJcgQciBHyGFijsgQFlQ53HanpJ+3/bO2Xybp3ZKub7csTBkyhBzIEXIgR6hs4khSROywfYakGyUtk/T5iNjQemWYGmQIOZAj5ECOUIfbuGJ07uHJlmrMvs5Z0+YV3Bning1tZkiazRy1sb/ss9FopLm5OfZFaGo+IkY7P8mM2wAAAAk0SQAAAAk0SQAAAAk0SQAAAAlV5kmqbe3atZqbm2tj1dnkPrmRE8EB9EHufdGsnQgOjGMkCQAAIIEmCQAAIIEmCQAAIIEmCQAAIIEmCQAAIIEmCQAAIIEmCQAAIGFik2T787afsH3vUhSE6USO0BQZQg7kCHVUGUm6TNKxLdeB6XeZyBGauUxkCM1dJnKEiiY2SRHxTUlPLkEtmGLkCE2RIeRAjlBHtsuS2F4vab0krV69OtdqMUPGMwQsFjlCU2QIC7KduB0RF0fEKCJGK1asyLVazJDxDHVdC4aLHKEpMoQF/LoNAAAggSYJAAAgocoUAP8s6duSftH2ZtsfbL8sTBtyhKbIEHIgR6hj4onbEXHKUhSC6UaO0BQZQg7kCHVwuA0AACCBJgkAACCBJgkAACCBJgkAACAh24zb4+bn52U72/oiItu6FuSsDwCmVRv7yjb26UAbGEkCAABIoEkCAABIoEkCAABIoEkCAABIoEkCAABIoEkCAABIqHKB21W2b7a90fYG22cuRWGYLuQITZEh5ECOUEeVeZJ2SPpYRNxle19J87Zvioj7Wq4N04UcoSkyhBzIESqbOJIUEVsi4q7y/rOSNkpa2XZhmC7kCE2RIeRAjlBHrRm3bR8u6ShJtydeWy9pfZaqMNV2lSMyhKrYFyEH9kWYxFWnh7e9j6RbJV0QEddOWDbrnPNclqSfIqL2h1g1R7kzhH5qM0PlsuSoh3Lu00ejkebm5tgXoan5iBjt/GSlX7fZ3lPSlyRdOWmnBOwKOUJTZAg5kCNUVeXXbZZ0qaSNEfHJ9kvCNCJHaIoMIQdyhDqqjCStk/Q+ScfYvqe8Hd9yXZg+5AhNkSHkQI5Q2cQTtyPiW5I4gQeNkCM0RYaQAzlCHcy4DQAAkECTBAAAkECTBAAAkECTBAAAkFBrxu2uMPHj7Fm7dq3m5ua6LmNJkXPMCrLeb7kncB7y981IEgAAQAJNEgAAQAJNEgAAQAJNEgAAQAJNEgAAQAJNEgAAQMLEJsn2XrbvsP0ftjfYPn8pCsP0IEPIgRwhB3KEOqrMk/QjScdExHbbe0r6lu0bIuI7LdeG6UGGkAM5Qg7kCJVNbJKimFVqe/lwz/KWd6YpTDUyhBzIEXIgR6ij0jlJtpfZvkfSE5JuiojbE8ustz1ne7amSUYldTO0devWpS8Svce+CDlMyhEZwgLXmX7c9gGSrpP00Yi4dzfL0ZXPgIioPdd81QyNRqPgsiTTbzEZktgX4cXazNEsZmhGL0syHxGjnZ+s9eu2iHhK0i2Sjs1UFGYMGUIO5Ag5kCNMUuXXbSvKblu295b0Nkn3t10YpgcZQg7kCDmQI9RR5ddth0i63PYyFU3V1RHx5XbLwpQhQ8iBHCEHcoTKqvy67buSjlqCWjClyBByIEfIgRyhDmbcBgAASKBJAgAASKBJAgAASKBJAgAASKjy6zZg8NqYzCz3hGsAMI2GsK/c1f8jGEkCAABIoEkCAABIoEkCAABIoEkCAABIoEkCAABIoEkCAABIqNwk2V5m+27bXAgQi0KGkAM5QlNkCFXVGUk6U9LGtgrBTCBDyIEcoSkyhEoqNUm2D5V0gqTPtVsOphUZQg7kCE2RIdRRdSTpQknnSPrJrhawvd72nO25LJVh2tTK0NatW5euMgwJ+yI0RYZQ2cQmyfaJkp6IiPndLRcRF0fEKCJG2arDVFhMhlasWLFE1WEo2BehKTKEuqqMJK2T9E7bmyRdJekY21e0WhWmDRlCDuQITZEh1OI6F56z/WZJZ0fEiROW6//V7NBYRNS+amzVDI1Go5ibyzfSPYQL3LZRY98tJkMS+yK8WJv7olnM0BAuSJub7fnUyCHzJAEAACTUGkmqvNIZ7Lxn0WJHAapgJGk2tJkhiX3RrGgzR7OYIUaSfoqRJAAAgASaJAAAgASaJAAAgASaJAAAgASaJAAAgITlXRcALIVZ/LUGAPTBkH+5y0gSAABAAk0SAABAAk0SAABAAk0SAABAAk0SAABAQqVft9neJOlZSS9I2pG6vgkwCTlCU2QIOZAjVFVnCoC3RMS21irBrCBHaIoMIQdyhIk43AYAAJBQtUkKSV+3PW97fZsFYaqRIzRFhpADOUIlVQ+3rYuIR22/WtJNtu+PiG+OL1AGjbBhd3abo/EMrV69uqsa0W/si5BD5X0RZpvrXq7B9nmStkfE3+1mGa4BMQMiYtFzzU/K0Wg0irm5ucWufpCGPHX/YrWZoXIZ9kUzoM0czWKGcl/GaSD7tvnUCfwTD7fZfqXtfRfuS3q7pHvz14dpRo7QFBlCDuQIdVQ53HaQpOvKTnC5pC9ExNdarQrTiByhKTKEHMgRKpvYJEXEQ5J+bQlqwRQjR2iKDCEHcoQ6mAIAAAAggSYJAAAggSYJAAAggSYJAAAggSYJAAAgofZkkpVWam+V9N8VFj1QUp8vMNj3+qTuajwsIla0tfIpypDU/xqnMkPSVOWo7/VJU5qjKcqQ1P8au6wvmaNWmqSqbM+lZrjsi77XJw2jxjYNYfv7XmPf61sKff8M+l6fNIwa2zSE7e97jX2sj8NtAAAACTRJAAAACV03SRd3/P6T9L0+aRg1tmkI29/3Gvte31Lo+2fQ9/qkYdTYpiFsf99r7F19nZ6TBAAA0FddjyQBAAD0Ek0SAABAQidNku1jbT9g+0Hb53ZRw+7YXmX7ZtsbbW+wfWbXNaXYXmb7bttf7rqWLvQ5R2RoGPqcIYkcDUWfc0SGmlnyJsn2MkmflnScpDWSTrG9ZqnrmGCHpI9FxC9L+k1Jf9zDGiXpTEkbuy6iCwPIERnquQFkSCJHvTeAHJGhBroYSTpa0oMR8VBEPC/pKkkndVDHLkXEloi4q7z/rIovbmW3Vb2Y7UMlnSDpc13X0pFe54gMDUKvMySRo4HodY7IUDNdNEkrJT089nizevaFjbN9uKSjJN3ebSUvcaGkcyT9pOtCOjKYHJGh3hpMhiRy1GODyREZqq+LJsmJ53o5D4HtfSR9SdJZEfFM1/UssH2ipCciYr7rWjo0iByRoV4bRIYkctRzg8gRGVqcLpqkzZJWjT0+VNKjHdSxW7b3VBGoKyPi2q7r2ck6Se+0vUnF0O4xtq/otqQl1/sckaHe632GJHI0AL3PERlavCWfTNL2ckn/Jemtkh6RdKek90TEhiUtZDdsW9Llkp6MiLO6rmd3bL9Z0tkRcWLXtSylvueIDPVf3zMkkaMh6HuOyFAzSz6SFBE7JJ0h6UYVJ5Bd3ZcwjVkn6X0qOtp7ytvxXReFnxpAjshQzw0gQxI56r0B5IgMNcBlSQAAABKYcRsAACCBJgkAACCBJgkAACCBJgkAACCBJgkAACCBJgkAACCBJgkAACDh/wCQDdKsiE6+cwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 720x288 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "#We have 4 basis images:\n",
    "N = 100\n",
    "D = 36\n",
    "fig, (ax1,ax2,ax3,ax4) = plt.subplots(1, 4,figsize = (10,4))\n",
    "b1 = np.array([[0,1,0,0,0,0],\n",
    "               [1,1,1,0,0,0],\n",
    "               [0,1,0,0,0,0],\n",
    "               [0,0,0,0,0,0],\n",
    "               [0,0,0,0,0,0],\n",
    "               [0,0,0,0,0,0]])\n",
    "\n",
    "ax1.imshow(b1,cmap = \"gray\")\n",
    "ax1.set_title(\"Basis Image 1\")\n",
    "\n",
    "\n",
    "b2 = np.array([[0,0,0,0,0,0],\n",
    "               [0,0,0,0,0,0],\n",
    "               [0,0,0,0,0,0],\n",
    "               [1,1,1,0,0,0],\n",
    "               [1,0,1,0,0,0],\n",
    "               [1,1,1,0,0,0]])\n",
    "\n",
    "ax2.imshow(b2,cmap = \"gray\")\n",
    "ax2.set_title(\"Basis Image 2\")\n",
    "\n",
    "\n",
    "b3 = np.array([[0,0,0,1,1,1],\n",
    "               [0,0,0,0,1,1],\n",
    "               [0,0,0,0,0,1],\n",
    "               [0,0,0,0,0,0],\n",
    "               [0,0,0,0,0,0],\n",
    "               [0,0,0,0,0,0]])\n",
    "\n",
    "ax3.imshow(b3,cmap = \"gray\")\n",
    "ax3.set_title(\"Basis Image 3\")\n",
    "\n",
    "b4 = np.array([[0,0,0,0,0,0],\n",
    "               [0,0,0,0,0,0],\n",
    "               [0,0,0,0,0,0],\n",
    "               [0,0,0,1,0,0],\n",
    "               [0,0,0,1,1,1],\n",
    "               [0,0,0,1,0,0]])\n",
    "\n",
    "ax4.imshow(b4,cmap = \"gray\")\n",
    "ax4.set_title(\"Basis Image 4\")\n",
    "pass\n",
    "\n",
    "b1 = b1.reshape(D)\n",
    "b2 = b2.reshape(D)\n",
    "b3 = b3.reshape(D)\n",
    "b4 = b4.reshape(D)\n",
    "sigmaX = 0.5\n",
    "\n",
    "#Create X from basis vectors\n",
    "zb1 = np.outer(np.random.binomial(1,.5,100),b1)\n",
    "zb2 = np.outer(np.random.binomial(1,.5,100),b2)\n",
    "zb3 = np.outer(np.random.binomial(1,.5,100),b3)\n",
    "zb4 = np.outer(np.random.binomial(1,.5,100),b4)\n",
    "\n",
    "X = zb1 + zb2 + zb3 + zb4\n",
    "#Add noise\n",
    "X = X + np.random.normal(0,sigmaX,(N,D))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Profile\n",
    "\n",
    "We adopt the decorator from a post on Medium written by Farhad Malik (Malik 2020) to profile our original python code.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cProfile\n",
    "import functools\n",
    "import pstats\n",
    "import tempfile\n",
    "def profile_me(func):\n",
    "    @functools.wraps(func)\n",
    "    def wraps(*args, **kwargs):\n",
    "        file = tempfile.mktemp()\n",
    "        profiler = cProfile.Profile()\n",
    "        profiler.runcall(func, *args, **kwargs)\n",
    "        profiler.dump_stats(file)\n",
    "        metrics = pstats.Stats(file)\n",
    "        metrics.strip_dirs().sort_stats('time').print_stats(10)\n",
    "    return wraps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@profile_me\n",
    "def sampler(X,alpha,niter,epsilon,sigma_X,sigma_A,alpha_a_prior,alpha_b_prior,max_new):\n",
    "    '''\n",
    "    This function performs a Gibbs sampler using the binary matrix Z generated by Indian buffet process and a \n",
    "    log likelihood function for the linear-Gaussian bindary latent feature model.\n",
    "    \n",
    "    The parameters are:\n",
    "    \n",
    "    X: Data matrix\n",
    "    \n",
    "    alpha: parameter for the Possion distribution that is used to generate a binary matrix Z using Indian buffet process\n",
    "    \n",
    "    niter: The number of iterations for the sampler\n",
    "    \n",
    "    Sigma_X: Standard deviation of X\n",
    "    \n",
    "    Sigma_A: Standard deviation of alpha\n",
    "    \n",
    "    alpha_a_prior: Shape hyperparameter for the prior distribution of alpha, which follows a Gamma distribution.\n",
    "    \n",
    "    alpha_b_prior: Rate hyperparameter for the prior distribution of alpha, which follows a Gamma distribution.\n",
    "\n",
    "    max_new: Maximum number of new K's per iteration\n",
    "    \n",
    "    '''\n",
    "    N = X.shape[0]\n",
    "    D = X.shape[1]\n",
    "    Z = IBP(alpha,N) # set inital Z\n",
    "    K = Z.shape[1]\n",
    "    K_values = np.zeros(niter)\n",
    "    alpha_values = np.zeros(niter)\n",
    "    Sigma_X_values = np.zeros(niter)\n",
    "    Sigma_A_values = np.zeros(niter)\n",
    "    HN = 0\n",
    "    for i in range(1,N+1):\n",
    "        HN += 1.0/i\n",
    "    for runs in range(niter):\n",
    "        for i in range(N):\n",
    "            for j in range(K):\n",
    "                #Sample Z given conditionals\n",
    "                \n",
    "                col_k_count = sum(Z[:,j]) - Z[i,j] #p(zik|z-ik) = 0 so we set to 0\n",
    "                if col_k_count == 0:\n",
    "                    Z[i,j] = 0\n",
    "                    \n",
    "                else:\n",
    "                    Z[i,j] = 0\n",
    "                    Z0_p = log_likelyhood(X,N,D,K,sigma_X,sigma_A,Z) + np.log(N - col_k_count)\n",
    "                    Z[i,j] = 1\n",
    "                    Z1_p = log_likelyhood(X,N,D,K,sigma_X,sigma_A,Z) + np.log(col_k_count)\n",
    "                    L = Z1_p - Z0_p\n",
    "                    if L > 40: #helps with overflow\n",
    "                        Z[i,j] = 1\n",
    "                    elif L < -40:\n",
    "                        Z[i,j] = 0\n",
    "                    elif np.exp(L)/(1 + np.exp(L)) > np.random.random():\n",
    "                        Z[i,j] = 1\n",
    "                    else:\n",
    "                        Z[i,j] = 0\n",
    "                        \n",
    "            #Sample to see if new columns get added\n",
    "            log_prob = np.zeros(max_new)\n",
    "            a_N = alpha/N\n",
    "            log_prob[0] = -a_N + log_likelyhood(X,N,D,Z.shape[1],sigma_X,sigma_A,Z)\n",
    "            for new_ks in range(1,max_new):\n",
    "                new_cols = np.zeros((N,new_ks))\n",
    "                new_cols[i,:] = 1\n",
    "                Z_new = np.hstack((Z,new_cols))\n",
    "                #Poisson(alpha/n) * log likelyhood\n",
    "                log_prob[new_ks] = new_ks*np.log(a_N) - a_N - np.log(math.factorial(new_ks)) + log_likelyhood(X,N,D,Z_new.shape[1],sigma_X,sigma_A,Z_new)\n",
    "            #Convert log likelyhoods\n",
    "            prob = np.exp(log_prob - max(log_prob))\n",
    "            prob = prob/sum(prob)\n",
    "\n",
    "            #Sample probabilites and add columns accordingly\n",
    "            new_cols_add = list(np.random.multinomial(1,prob) == 1).index(1)\n",
    "            col_k_count = np.sum(Z,axis = 0) - Z[i,:]\n",
    "            if new_cols_add == 0:\n",
    "                Z = Z[:,col_k_count != 0]\n",
    "            else:\n",
    "                newcols = np.zeros((N,new_cols_add))\n",
    "                newcols[i,:] = 1\n",
    "                Z = np.hstack((Z[:,col_k_count != 0],newcols))\n",
    "            K = Z.shape[1]\n",
    "        \n",
    "        #Part2\n",
    "        current_likelyhood = log_likelyhood(X,N,D,K,sigma_X,sigma_A,Z) \n",
    "        \n",
    "        #Sigma_X\n",
    "        sigma_X_new = sigma_X + np.random.uniform(-epsilon,epsilon)\n",
    "        new_likelyhood = log_likelyhood(X,N,D,K,sigma_X_new,sigma_A,Z)\n",
    "        if new_likelyhood - current_likelyhood >= 0:\n",
    "            sigma_X = sigma_X_new\n",
    "        elif np.exp(new_likelyhood - current_likelyhood) > np.random.random():\n",
    "            sigma_X = sigma_X_new\n",
    "        else:\n",
    "            sigma_X = sigma_X\n",
    "            \n",
    "        #Sigma_A\n",
    "        sigma_A_new = sigma_A + np.random.uniform(-epsilon,epsilon)\n",
    "        new_log_likelyhood = log_likelyhood(X,N,D,K,sigma_X,sigma_A_new,Z)\n",
    "        if new_likelyhood - current_likelyhood >= 0:\n",
    "            sigma_A = sigma_A_new\n",
    "        elif np.exp(new_likelyhood - current_likelyhood) > np.random.random():\n",
    "            sigma_A = sigma_A_new\n",
    "        else:\n",
    "            sigma_A = sigma_A\n",
    "         \n",
    "        #Alpha\n",
    "        alpha = np.random.gamma(alpha_a_prior + K,alpha_b_prior + 1/(1 + HN))\n",
    "        \n",
    "        K_values[runs] = K\n",
    "        alpha_values[runs] = alpha\n",
    "        Sigma_X_values[runs] = sigma_X\n",
    "        Sigma_A_values[runs] = sigma_A\n",
    "        # print(runs,K,sigma_X)\n",
    "    return(K_values,alpha_values,Sigma_X_values,Sigma_A_values,Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Apr 26 20:29:07 2021    C:\\Users\\peyto\\AppData\\Local\\Temp\\tmpqrij47xi\n",
      "\n",
      "         60448050 function calls (59847646 primitive calls) in 178.538 seconds\n",
      "\n",
      "   Ordered by: internal time\n",
      "   List reduced from 65 to 10 due to restriction <10>\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "   979552   78.063    0.000  142.160    0.000 IBP_Sampler.py:43(log_likelyhood)\n",
      "        1   15.430   15.430  178.538  178.538 <ipython-input-6-57a98512a3da>:1(sampler)\n",
      "   979552    8.844    0.000   17.554    0.000 linalg.py:482(inv)\n",
      "   979552    7.920    0.000   16.040    0.000 linalg.py:2063(det)\n",
      "   388480    7.698    0.000    7.698    0.000 {built-in method builtins.sum}\n",
      "  1959104    7.667    0.000   13.986    0.000 twodim_base.py:154(eye)\n",
      "   979552    7.096    0.000    7.096    0.000 {method 'trace' of 'numpy.ndarray' objects}\n",
      "  2359311    6.959    0.000    6.959    0.000 {built-in method numpy.zeros}\n",
      "4139726/3539322    4.841    0.000   54.407    0.000 {built-in method numpy.core._multiarray_umath.implement_array_function}\n",
      "  1959104    3.711    0.000    6.415    0.000 linalg.py:144(_commonType)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(10)\n",
    "sampler(X,1,1000,.05,1,1,1,1,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the profile above, we can see that most of the runtime was spent on the `log_likelyhood` function. Therefore, we concentrated our energy to explore how to optimize the performance of our `log_likelyhood` function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Optimization of matrix calculations\n",
    "\n",
    "First, we explored ways to optimize the `log_likelyhood` function by calculating the determinant and inverse of matrix $\\textbf{M}$ differently.\n",
    "\n",
    "We define a `sampler_test` function that allows us to use different log likelyhood functions as input. We test each method using the same inputs with `niter = 100`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampler_test(X,alpha,niter,epsilon,sigma_X,sigma_A,alpha_a_prior,alpha_b_prior,max_new, log_likelyhood):\n",
    "    '''\n",
    "    This function allows users to implement the Gibbs sampler using different log likelihood functions.\n",
    "    \n",
    "    The parameters are the same as the sampler function except that we need to input a function for the log_likelyhood function\n",
    "    \n",
    "    X: Data matrix\n",
    "    \n",
    "    alpha: parameter for the Possion distribution that is used to generate a binary matrix Z using Indian buffet process\n",
    "    \n",
    "    niter: The number of iterations for the sampler\n",
    "    \n",
    "    Sigma_X: Standard deviation of X\n",
    "    \n",
    "    Sigma_A: Standard deviation of alpha\n",
    "    \n",
    "    alpha_a_prior: Shape hyperparameter for the prior distribution of alpha, which follows a Gamma distribution.\n",
    "    \n",
    "    alpha_b_prior: Rate hyperparameter for the prior distribution of alpha, which follows a Gamma distribution.\n",
    "\n",
    "    max_new: Maximum number of new K's per iteration\n",
    "    \n",
    "    log_likelyhood: a log likelihood function\n",
    "    \n",
    "    '''\n",
    "    N = X.shape[0]\n",
    "    D = X.shape[1]\n",
    "    Z = IBP(alpha,N) # set inital Z\n",
    "    K = Z.shape[1]\n",
    "    K_values = np.zeros(niter)\n",
    "    alpha_values = np.zeros(niter)\n",
    "    Sigma_X_values = np.zeros(niter)\n",
    "    Sigma_A_values = np.zeros(niter)\n",
    "    HN = 0\n",
    "    for i in range(1,N+1):\n",
    "        HN += 1.0/i\n",
    "    for runs in range(niter):\n",
    "        for i in range(N):\n",
    "            for j in range(K):\n",
    "                #Sample Z given conditionals\n",
    "                \n",
    "                col_k_count = sum(Z[:,j]) - Z[i,j] #p(zik|z-ik) = 0 so we set to 0\n",
    "                if col_k_count == 0:\n",
    "                    Z[i,j] = 0\n",
    "                    \n",
    "                else:\n",
    "                    Z[i,j] = 0\n",
    "                    Z0_p = log_likelyhood(X, N,D,K,sigma_X,sigma_A,Z) + np.log(N - col_k_count)\n",
    "                    Z[i,j] = 1\n",
    "                    Z1_p = log_likelyhood(X, N,D,K,sigma_X,sigma_A,Z) + np.log(col_k_count)\n",
    "                    L = Z1_p - Z0_p\n",
    "                    if L > 40: #helps with overflow\n",
    "                        Z[i,j] = 1\n",
    "                    elif L < -40:\n",
    "                        Z[i,j] = 0\n",
    "                    elif np.exp(L)/(1 + np.exp(L)) > np.random.random():\n",
    "                        Z[i,j] = 1\n",
    "                    else:\n",
    "                        Z[i,j] = 0\n",
    "                        \n",
    "            #Sample to see if new columns get added\n",
    "            log_prob = np.zeros(max_new)\n",
    "            a_N = alpha/N\n",
    "            log_prob[0] = -a_N + log_likelyhood(X, N,D,Z.shape[1],sigma_X,sigma_A,Z)\n",
    "            for new_ks in range(1,max_new):\n",
    "                new_cols = np.zeros((N,new_ks))\n",
    "                new_cols[i,:] = 1\n",
    "                Z_new = np.hstack((Z,new_cols))\n",
    "                #Poisson(alpha/n) * log likelyhood\n",
    "                log_prob[new_ks] = new_ks*np.log(a_N) - a_N - np.log(math.factorial(new_ks)) + log_likelyhood(X, N,D,Z_new.shape[1],sigma_X,sigma_A,Z_new)\n",
    "            #Convert log likelyhoods\n",
    "            prob = np.exp(log_prob - max(log_prob))\n",
    "            prob = prob/sum(prob)\n",
    "\n",
    "            #Sample probabilites and add columns accordingly\n",
    "            new_cols_add = list(np.random.multinomial(1,prob) == 1).index(1)\n",
    "            col_k_count = np.sum(Z,axis = 0) - Z[i,:]\n",
    "            if new_cols_add == 0:\n",
    "                Z = Z[:,col_k_count != 0]\n",
    "            else:\n",
    "                newcols = np.zeros((N,new_cols_add))\n",
    "                newcols[i,:] = 1\n",
    "                Z = np.hstack((Z[:,col_k_count != 0],newcols))\n",
    "            K = Z.shape[1]\n",
    "        \n",
    "        #Part2\n",
    "        current_likelyhood = log_likelyhood(X, N,D,K,sigma_X,sigma_A,Z) \n",
    "        \n",
    "        #Sigma_X\n",
    "        sigma_X_new = sigma_X + np.random.uniform(-epsilon,epsilon)\n",
    "        new_likelyhood = log_likelyhood(X, N,D,K,sigma_X_new,sigma_A,Z)\n",
    "        if new_likelyhood - current_likelyhood >= 0:\n",
    "            sigma_X = sigma_X_new\n",
    "        elif np.exp(new_likelyhood - current_likelyhood) > np.random.random():\n",
    "            sigma_X = sigma_X_new\n",
    "        else:\n",
    "            sigma_X = sigma_X\n",
    "            \n",
    "        #Sigma_A\n",
    "        sigma_A_new = sigma_A + np.random.uniform(-epsilon,epsilon)\n",
    "        new_log_likelyhood = log_likelyhood(X, N,D,K,sigma_X,sigma_A_new,Z)\n",
    "        if new_likelyhood - current_likelyhood >= 0:\n",
    "            sigma_A = sigma_A_new\n",
    "        elif np.exp(new_likelyhood - current_likelyhood) > np.random.random():\n",
    "            sigma_A = sigma_A_new\n",
    "        else:\n",
    "            sigma_A = sigma_A\n",
    "         \n",
    "        #Alpha\n",
    "        alpha = np.random.gamma(alpha_a_prior + K,alpha_b_prior + 1/(1 + HN))\n",
    "        \n",
    "        K_values[runs] = K\n",
    "        alpha_values[runs] = alpha\n",
    "        Sigma_X_values[runs] = sigma_X\n",
    "        Sigma_A_values[runs] = sigma_A\n",
    "        \n",
    "#         if (runs % 100 == 0):\n",
    "#             print(runs,K,sigma_X)\n",
    "    return(K_values,alpha_values,Sigma_X_values,Sigma_A_values,Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.7 s ± 159 ms per loop (mean ± std. dev. of 2 runs, 2 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -r2 -n2 sampler_test(X,1,100,.05,1,1,1,1,4, log_likelyhood = log_likelyhood)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.1 Replacing `inv` with `solve`\n",
    "\n",
    "In this approach, we consider replacing the calculation of inverse matrix with linear solve. Our intuition is linear solve may require a smaller complexity than calculating the inverse of matrix $\\textbf{M}$. We have to calculate $\\textbf{Z} \\textbf{M}^{-1} \\textbf{Z}^T$. So instead of calculating `Z @ np.linalg.inv(M) @ Z.T`, we tried `Z @ np.linalg.solve(M, Z.T)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_likelyhood_op_solve(X, N,D,K,sigma_X,sigma_A,Z):\n",
    "    '''\n",
    "    Log likelyhood function where inverse function is replaced by linear solve\n",
    "    '''    \n",
    "    M = Z.T @ Z + (sigma_X**2/sigma_A**2)*np.eye(K)\n",
    "\n",
    "    part1 = N*D/2 * np.log(2*np.pi) + (N - K)*D*np.log(sigma_X) + K*D*np.log(sigma_A)+D/2*np.log(np.linalg.det(M))\n",
    "\n",
    "    part2_inside = np.eye(N) - (Z @ np.linalg.solve(M, Z.T))\n",
    "    part2 = -1/(2 * sigma_X**2) * np.trace(X.T @ part2_inside @ X)\n",
    "    return part2 - part1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.3 s ± 260 ms per loop (mean ± std. dev. of 2 runs, 2 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -r2 -n2 sampler_test(X,1,100,.05,1,1,1,1,4, log_likelyhood = log_likelyhood_op_solve)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We found the performance is comparable. Using `np.linalg.solve` may not have a noticeable improvement for the function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.2 Singular Value Decomposition\n",
    "\n",
    "We also explored a way of computing the determinant and the inverse matrix using singular value decomposition. Since $\\textbf{Z}^T \\textbf{Z} + \\frac{\\sigma_X^2}{\\sigma_A^2}\\textbf{I}$ is a nonsingular matrix, we can calculate the determinant by calculating the products of the singular values $s$. Once we have $U$ and $V^T$ from the singular value decomposition, we can also calculate the inverse of matrix $\\textbf{M}$ as $V D^{-1} U^T$ where $D^{-1}$ is a diagonal matrix with diagonal elements $1/s$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_likelyhood_op_svd(X, N,D,K,sigma_X,sigma_A,Z):\n",
    "    '''\n",
    "    Log likelyhood function using singular value decomposition\n",
    "    '''\n",
    "    M = Z.T @ Z + np.diag([sigma_X**2/sigma_A**2]*K)\n",
    "    U, s, Vt = np.linalg.svd(M)\n",
    "    det_M = np.prod(s)\n",
    "    part1 = N*D/2 * np.log(2*np.pi) + (N - K)*D*np.log(sigma_X) + K*D*np.log(sigma_A)+D/2*np.log(det_M)\n",
    "    \n",
    "    M_inv = Vt.T @ np.diag(1/s) @ U.T\n",
    "    part2_inside = np.eye(N) - (Z @ M_inv @ Z.T)\n",
    "    part2 = -1/(2 * sigma_X**2) * np.trace(X.T @ part2_inside @ X)\n",
    "    return part2 - part1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.4 s ± 368 ms per loop (mean ± std. dev. of 2 runs, 2 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -r2 -n2 sampler_test(X,1,100,.05,1,1,1,1,4, log_likelyhood = log_likelyhood_op_svd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But the performance is comparable to the original likelyhood function. It may be a bit slower than the original function. This may be due to the computational time required by finding the singular value decomposition. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.3 Using functools\n",
    "\n",
    "We also tried to use the `reduce` function from `functools` to see if we can improve the performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_likelyhood_op_reduce(X, N,D,K,sigma_X,sigma_A,Z):\n",
    "    '''\n",
    "    Log likelyhood function using 'reduce' function from functools\n",
    "    ''' \n",
    "    M = Z.T @ Z + np.diag(K*[(sigma_X**2/sigma_A**2)])\n",
    "    part1 = N*D/2 * np.log(2*np.pi) + (N - K)*D*np.log(sigma_X) + K*D*np.log(sigma_A)+D/2*np.log(np.linalg.det(M))\n",
    "    part2_inside = np.eye(N) - reduce(np.matmul, [Z, np.linalg.inv(M), Z.T])\n",
    "    part2 = -1/(2 * sigma_X**2) * np.trace( reduce(np.matmul, [X.T, part2_inside, X]))\n",
    "    return part2 - part1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.3 s ± 17.2 ms per loop (mean ± std. dev. of 2 runs, 2 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -r2 -n2 sampler_test(X,1,100,.05,1,1,1,1,4, log_likelyhood = log_likelyhood_op_reduce)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, we did not find any noticeable improvement in speed. Therefore, we decide to explore optimization methods using cython and numba with the original `log_likelyhood` function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Cython and Numba\n",
    "\n",
    "Next, we tried to optimize the `log_likelyhood` function using cython. We compared the speed of doing matrix multiplication using the `@` operator in numpy with a matrix multiplication function written in cython. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cython\n",
    "\n",
    "import cython\n",
    "import numpy as np\n",
    "from libc.math cimport log, pi\n",
    "\n",
    "@cython.boundscheck(False)\n",
    "@cython.wraparound(False)\n",
    "def matrix_multiply(double[:,:] u, double[:, :] v):\n",
    "    '''\n",
    "    Matrix multiplication written in Cython\n",
    "    '''\n",
    "    cdef int i, j, k\n",
    "    cdef int m, n, p\n",
    "    cdef double temp\n",
    "    cdef double[:,:] res\n",
    "\n",
    "    m = u.shape[0]\n",
    "    n = u.shape[1]\n",
    "    p = v.shape[1]\n",
    "    \n",
    "    res = np.zeros((m,p))\n",
    "\n",
    "    with cython.nogil: \n",
    "        for i in range(m):\n",
    "            for j in range(p):\n",
    "                temp = 0\n",
    "                for k in range(n):\n",
    "                    temp += u[i,k] * v[k,j]\n",
    "                res[i,j] = temp\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.1 Comparing the performance of matrix multiplication\n",
    "\n",
    "First, we compared the performance for matrix multiplication between the `@` operator in numpy and the `matrix_multiply` function we wrote in cython."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the performance of our log_likeyhood function\n",
    "alpha=1\n",
    "Z = IBP(alpha,N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_1 = np.random.randn(1000,36)\n",
    "A_2 = np.random.randn(36,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28.7 ms ± 674 µs per loop (mean ± std. dev. of 7 runs, 7 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -r7 -n7 matrix_multiply(A_1,A_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.14 ms ± 310 µs per loop (mean ± std. dev. of 7 runs, 7 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -r7 -n7 A_1 @ A_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We found that our `matrix_multiply` function written in cython is much slower than the `@` operator in numpy. Since matrix multiplications take a considerable amount of time in the `log_likelyhood` function, we decided not to use `matrix_multiply` function but wrote a cythonized `log_likelyhood` function using the `matmul` function in numpy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.2 Cythonize the `log_likelyhood` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cython\n",
    "\n",
    "import cython\n",
    "import numpy as np\n",
    "from libc.math cimport log, pi\n",
    "\n",
    "@cython.boundscheck(False)\n",
    "@cython.wraparound(False)\n",
    "\n",
    "def log_likelyhood_op_cy(double[:,:] X, int N, int D, int K, double sigma_X, double sigma_A, double[:,:] Z):\n",
    "    '''\n",
    "    Log likelyhood function written in cython\n",
    "    '''\n",
    "    cdef double[:,:] Zt = Z.T\n",
    "    cdef double[:,:] Xt = X.T\n",
    "    cdef double[:,:] M\n",
    "    cdef double part1\n",
    "    cdef double part2\n",
    "    cdef double res\n",
    "\n",
    "    M = np.matmul(Zt, Z)\n",
    "\n",
    "    for i in range(M.shape[0]):\n",
    "        M[i,i] = M[i,i] + sigma_X**2/sigma_A**2\n",
    "\n",
    "    part1 = N*D/2 * log(2*pi) + (N - K)*D*log(sigma_X) + K*D*log(sigma_A)+D/2*log(np.linalg.det(M))\n",
    "\n",
    "    M_ZT = np.matmul(np.linalg.inv(M), Z.T)\n",
    "    part2_inside = np.eye(N) - np.matmul(Z, M_ZT)\n",
    "\n",
    "    XT_P2 = np.matmul(Xt, part2_inside)\n",
    "    part2 = -1/(2 * sigma_X**2) * np.trace(np.matmul(XT_P2, X))\n",
    "\n",
    "    res = part2 - part1\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.1 s ± 416 ms per loop (mean ± std. dev. of 2 runs, 2 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -r2 -n2 sampler_test(X,1,100,.05,1,1,1,1,4, log_likelyhood = log_likelyhood_op_cy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the comparison above, cythonizing the liklyhood function does not outperfrom the original likelyhood function. One of the reasons can be our cython code still depends heavily on functions in the numpy package to compute the determinant and the inverse function. Using numpy functions in cython may create unnecessary overhead. Due to the complex data structures in cython, we chose to stick with the original `likelyhood` function for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.3 Numba\n",
    "\n",
    "We also tried to use numba for our matrix multiplication. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numba\n",
    "from numba import jit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit('double[:,:](double[:,:], double[:,:])')\n",
    "def matrix_multiply_numba2(A, B):\n",
    "    '''\n",
    "    Matrix multiplication using numba\n",
    "    '''\n",
    "    m, n = A.shape\n",
    "    n, p = B.shape\n",
    "    C = np.zeros((m, p))\n",
    "    for i in range(m):\n",
    "        for j in range(p):\n",
    "            d = 0.0\n",
    "            for k in range(n):\n",
    "                d += A[i,k] * B[k, j]\n",
    "            C[i,j] = d\n",
    "    return C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31.6 ms ± 2.45 ms per loop (mean ± std. dev. of 7 runs, 7 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -r7 -n7 matrix_multiply_numba2(A_1,A_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the result above, we can see that the speed of matrix multiplication using numba is much slower than using `@` operator in numpy. We can conclude that numba is not an optimal choice for our algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Conclusion on optimization\n",
    "\n",
    "The table below shows a brief summary of the average speed of different methods when we tested them with `niter = 100`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Original</th>\n",
       "      <th>Linear_solve</th>\n",
       "      <th>SVD</th>\n",
       "      <th>Reduce</th>\n",
       "      <th>Cython</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Speed (seconds)</th>\n",
       "      <td>11.7</td>\n",
       "      <td>12.3</td>\n",
       "      <td>14.4</td>\n",
       "      <td>12.3</td>\n",
       "      <td>13.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Original  Linear_solve   SVD  Reduce  Cython\n",
       "Speed (seconds)      11.7          12.3  14.4    12.3    13.1"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(dict(Original = 11.7,\n",
    "                       Linear_solve = 12.3, \n",
    "                       SVD = 14.4,\n",
    "                       Reduce = 12.3,\n",
    "                       Cython = 13.1),\n",
    "                  index = [\"Speed (seconds)\"]\n",
    "                 )\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the results, we did not find any method that provides a significant improvment for the speed of the algorithm. Therefore, we decided to use the original function written with numpy as final algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Reference\n",
    "\n",
    "Malik, Farhad. “Advanced Python: Learn How To Profile Python Code.” Medium, FinTechExplained, 20 July 2020, medium.com/fintechexplained/advanced-python-learn-how-to-profile-python-code-1068055460f9. \n",
    "\n",
    "Yildirim, Ilker. \"Bayesian Statistics: Indian Buffet Process.\" August 2012, https://www2.bcs.rochester.edu/sites/jacobslab/cheat_sheet/IndianBuffetProcess.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
